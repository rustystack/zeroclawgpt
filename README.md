# zeroclawgpt ğŸ¦€âš¡

> Zero-dependency GPT in pure Rust â€” a faithful port of Karpathy's microGPT.py that trains ~4,500x faster.

A complete, from-scratch GPT implementation in a single Rust file. No crates. No `ndarray`. No `rand`. No `serde`. Just `std` â€” and it generates real human names after training for less than a second.

```
step     0  loss=3.2943  | m<BOS>kv<BOS>tpl  kygocwfv  vydcdhlm
step  2500  loss=2.1965  | logan  leo  lagan
step  4999  loss=0.6869  | naomi  eleanora  ryan

Done in 0.689s
```

---

## Table of Contents

- [Why This Exists](#why-this-exists)
- [Quick Start](#quick-start)
- [How It Works](#how-it-works)
- [Architecture](#architecture)
- [Hyperparameters](#hyperparameters)
- [Training Output](#training-output)
- [Performance](#performance)
- [5 Bugs We Fixed](#5-bugs-we-fixed)
- [Why Zero Dependencies](#why-zero-dependencies)
- [Code Tour](#code-tour)
- [Extending It](#extending-it)
- [License](#license)

---

## Why This Exists

Andrej Karpathy released [microGPT.py](https://github.com/karpathy/microgpt) â€” a minimal GPT that fits in ~200 lines of Python with a custom scalar autograd engine. It trains on a list of baby names and learns to generate new ones.

We ported it to Rust. Faithfully. Then we read his actual source code carefully and found 5 meaningful differences in our implementation. Fixing them took us from generating `ioeanaa` to generating `naomi`.

The result: **474 lines of Rust, zero dependencies, 3,632 parameters, and a ~4,500x speedup over Python.**

---

## Quick Start

### Prerequisites

- [Rust](https://rustup.rs/) (1.56+ for edition 2021)

### Build & Run

```bash
git clone https://github.com/rustystack/zeroclawgpt.git
cd zeroclawgpt
cargo build --release
./target/release/zeroclawgpt
```

That's it. No data files to download â€” the training data (92 baby names) is embedded in the source.

### What You'll See

```
zeroclawgpt v2  vocab=27  params=3632  layers=1  embd=16  heads=4
Fixes: KV-cache causal attn | LR linear decay | beta2=0.95 | zero-init wo/fc2
Training 5000 steps

step     0  loss=3.2943  t=0.00s  | sample 0: m<BOS>kv<BOS>tpl  sample 1: kygocwfv  ...
step   500  loss=2.3344  t=0.07s  | sample 0: eyy  sample 1: iyne  ...
step  1000  loss=2.3421  t=0.14s  | sample 0: rarloeba  sample 1: alievy  ...
step  2000  loss=1.6842  t=0.27s  | sample 0: lmye  sample 1: rync  sample 2: luke  ...
step  3000  loss=1.0547  t=0.41s  | sample 0: carey  sample 1: gadrel  ...
step  4000  loss=1.2236  t=0.55s  | sample 0: jaden  sample 1: caleb  sample 2: axel  ...
step  4999  loss=0.6869  t=0.69s  | sample 0: naomi  sample 1: eleanora  sample 2: ryan  ...

Done in 0.689s
```

Watch the samples evolve from random noise â†’ plausible letter combos â†’ real names.

---

## How It Works

### The Task

Given a dataset of 92 names (`emma`, `oliver`, `luna`, `axel`, ...), train a tiny transformer to predict the next character. At inference time, feed it a `<BOS>` (beginning of sequence) token and let it generate characters one by one until it produces `<EOS>` (end of sequence).

### The Training Loop

Each of the 5,000 training steps:

1. **Pick a name** â€” cycle through the 92 names round-robin
2. **Tokenize** â€” convert to `[<BOS>, c1, c2, ..., <EOS>]` (character-level, max 8 tokens)
3. **Forward** â€” process each token position through the transformer, building up a KV cache so each position attends to all previous positions (causal attention)
4. **Loss** â€” cross-entropy: for each position, how surprised was the model by the actual next character?
5. **Backward** â€” compute analytical gradients for every parameter (no autograd graph)
6. **Update** â€” Adam optimizer with linear learning rate decay

### Inference

Generation is autoregressive:

```
Input:  <BOS>
Step 1: <BOS> â†’ predict 'n' â†’ "n"
Step 2: <BOS>, n â†’ predict 'a' â†’ "na"
Step 3: <BOS>, n, a â†’ predict 'o' â†’ "nao"
Step 4: <BOS>, n, a, o â†’ predict 'm' â†’ "naom"
Step 5: <BOS>, n, a, o, m â†’ predict 'i' â†’ "naomi"
Step 6: <BOS>, n, a, o, m, i â†’ predict <EOS> â†’ done!
```

Each step, the model sees all previous characters via the KV cache and predicts the next one.

---

## Architecture

```
token_id â”€â”€â†’ wte[tok]  â”€â”
                         â”œâ”€â”€â†’ x = tok_emb + pos_emb
pos_id   â”€â”€â†’ wpe[pos]  â”€â”˜
                â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚       Transformer Block Ã—1           â”‚
      â”‚                                      â”‚
      â”‚  x_res = x                           â”‚
      â”‚  x = RMSNorm(x)                      â”‚
      â”‚                                      â”‚
      â”‚  Q = x @ Wq                          â”‚
      â”‚  K = x @ Wk  â”€â”€â†’ append to KV cache  â”‚
      â”‚  V = x @ Wv  â”€â”€â†’ append to KV cache  â”‚
      â”‚                                      â”‚
      â”‚  â”Œâ”€ For each of 4 heads: â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚  â”‚  scores = QÂ·K^T / âˆšd_head       â”‚ â”‚
      â”‚  â”‚  weights = softmax(scores)       â”‚ â”‚
      â”‚  â”‚  head_out = weights Â· V          â”‚ â”‚
      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â”‚                                      â”‚
      â”‚  attn_out = concat(heads) @ Wo       â”‚
      â”‚  x = x_res + attn_out               â”‚
      â”‚                                      â”‚
      â”‚  x_res = x                           â”‚
      â”‚  x = RMSNorm(x)                      â”‚
      â”‚  x = x @ Wfc1                        â”‚
      â”‚  x = squared_relu(x)                 â”‚
      â”‚  x = x @ Wfc2                        â”‚
      â”‚  x = x_res + x                       â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
      logits = x @ Wte^T        (weight-tied with token embeddings)
                â”‚
      probs  = softmax(logits)
                â”‚
      loss   = -log(probs[target]) / seq_len
```

Key design choices (matching Karpathy's implementation):
- **RMSNorm** instead of LayerNorm (no bias, no learnable scale)
- **Squared ReLU** activation in the MLP (`max(0, x)Â²`)
- **Weight tying** â€” the output projection reuses the token embedding matrix
- **Zero-initialized** output projections (Wo, Wfc2) â€” residual stream starts as identity

---

## Hyperparameters

| Parameter | Value | Description |
|---|---|---|
| `N_EMBD` | 16 | Embedding dimension |
| `N_HEAD` | 4 | Number of attention heads |
| `HEAD_DIM` | 4 | Per-head dimension (`N_EMBD / N_HEAD`) |
| `N_LAYER` | 1 | Number of transformer blocks |
| `BLOCK_SIZE` | 8 | Maximum sequence length |
| `N_STEPS` | 5000 | Training iterations |
| `LR` | 1e-2 | Initial learning rate |
| `BETA1` | 0.9 | Adam first moment decay |
| `BETA2` | 0.95 | Adam second moment decay |
| LR schedule | linear decay | `LR Ã— (1 - step/N_STEPS)` â†’ decays to 0 |

All values match Karpathy's defaults exactly.

---

## Training Output

The model generates 5 samples every 500 steps. Here's what the learning progression looks like:

| Step | Loss | Sample Names | What's Happening |
|---|---|---|---|
| 0 | 3.29 | `m<BOS>kv<BOS>tpl` | Random noise â€” loss â‰ˆ -log(1/27) as expected |
| 500 | 2.33 | `eyy`, `iyne`, `adee` | Learning vowel/consonant patterns |
| 1000 | 2.34 | `rarloeba`, `alievy` | Longer sequences, still garbled |
| 2000 | 1.68 | `luke`, `rync` | First real name appears! |
| 2500 | 2.20 | `logan`, `leo` | More real names emerging |
| 3500 | 1.24 | `cole`, `kole` | Variations on learned patterns |
| 4000 | 1.22 | `jaden`, `caleb`, `axel` | Consistent real names |
| 4999 | 0.69 | `naomi`, `eleanora`, `ryan` | Strong generation quality |

---

## Performance

| Metric | Karpathy Python | zeroclawgpt |
|---|---|---|
| 1000-step training time | 297.7s | **0.065s** |
| Full 5000-step run | ~25 min | **< 1s** |
| Speedup | 1Ã— | **~4,580Ã—** |
| Parameters | 3,632 | 3,632 |
| Final loss | ~2.4 (1000 steps) | **0.69** (5000 steps) |
| Memory allocations per step | Tens of thousands | Minimal |

### Why So Fast?

Karpathy's Python builds a **dynamic computation graph** â€” every scalar `float` is wrapped in a `Value` object with a `_backward` closure for autograd. At 3,632 parameters, one forward pass creates tens of thousands of heap-allocated nodes that must be topologically sorted and traversed for backprop.

We implement **analytical matrix-level gradients** â€” the same math, computed directly:

| Operation | Python (autograd) | Rust (analytical) |
|---|---|---|
| `c = a + b` | Allocate node + closure | `dc = 1`, applied inline |
| `softmax â†’ CE` | Chain of exp/sum/log nodes | `d_logits = probs - one_hot` |
| RMSNorm backward | Graph traversal | 4-line function |
| Full attention backward | Thousands of nodes | Direct matrix ops |

---

## 5 Bugs We Fixed

We started with a naive port, then read Karpathy's actual source line by line. Five differences emerged:

### 1. ğŸ”´ KV Cache â€” Real Causal Attention (Critical)

**The bug:** Our v1 processed each position independently. Token at position 3 could only attend to *itself* â€” not tokens 0, 1, 2. This is fundamentally not a language model.

**The fix:** Accumulate keys and values into a growing cache. At position `t`, the model attends over all positions `[0..t]`, exactly like Karpathy's implementation.

**Impact:** This is the difference between `ioeanaa` and `naomi`.

### 2. ğŸŸ¡ Adam beta2: 0.999 â†’ 0.95

Lower `beta2` means the optimizer's second moment estimate adapts faster â€” it forgets old gradient magnitudes more quickly. On a tiny dataset with few training steps, this converges noticeably faster.

### 3. ğŸŸ¡ Linear LR Decay

**The bug:** Constant learning rate throughout training.

**The fix:** `lr = LR Ã— (1 - step/N_STEPS)`, decaying linearly to zero. Prevents overshooting near convergence.

### 4. ğŸŸ¡ Zero-Init Output Projections

**The bug:** `Wo` and `Wfc2` initialized with `std=0.02` like other weights.

**The fix:** Initialize to zero. This means at step 0, both the attention and MLP blocks contribute *nothing* â€” the residual stream is pure identity. The model only starts deviating as gradients flow in. This is the GPT-2 "scaled initialization" technique.

### 5. ğŸŸ¢ Loss Normalization

**The bug:** Gradients were `(seq_len-1)Ã—` too large â€” we normalized loss for display but not before the backward pass.

**The fix:** Scale `d_logits` by `1/(seq_len-1)` before backpropagation, matching Karpathy's normalization.

---

## Why Zero Dependencies

This isn't just a flex. It's the point.

Karpathy's microGPT uses no ML frameworks â€” no PyTorch, no JAX. Just a tiny autograd engine that fits in the same file. The beauty is seeing every piece of a GPT laid bare with nothing hidden.

We carry that philosophy to Rust:

- **PRNG** â€” xoshiro128+ in 15 lines, with Box-Muller gaussian sampling
- **Linear algebra** â€” row-major matrix multiply, element-wise ops
- **Optimizer** â€” Adam with bias correction, implemented directly
- **Gradients** â€” analytical, not autograd. Every backward function is hand-derived

Python's `random` module is ~2,000 lines of C that Karpathy doesn't count. We don't count our 15-line PRNG either. Fair's fair.

**The entire model â€” forward pass, backward pass, optimizer, data loading, inference â€” is one file you can read top to bottom in 20 minutes.**

---

## Code Tour

The source (`src/main.rs`) is organized in sections:

| Lines | Section | What It Does |
|---|---|---|
| 1â€“10 | Header | Constants, imports |
| 11â€“18 | Hyperparameters | All tuneable values in one place |
| 20â€“50 | PRNG | xoshiro128+ RNG, gaussian sampling, categorical sampler |
| 52â€“85 | Matrix ops | `linear()`, `softmax()`, `rmsnorm()` and their backward passes |
| 87â€“140 | Model struct | Parameter storage, gradient buffers, Adam optimizer |
| 142â€“180 | Activation cache | Per-position saved state for backward pass |
| 182â€“260 | Forward pass | Embeddings â†’ attention with KV cache â†’ MLP â†’ logits |
| 262â€“360 | Backward pass | Analytical gradients through every operation |
| 362â€“395 | Data & vocab | Baby names dataset, character-level tokenizer |
| 397â€“420 | Inference | Autoregressive generation with KV cache |
| 422â€“475 | Main | Training loop with logging |

### Key functions

- **`forward()`** â€” processes one token position, appends to KV cache, returns cached activations
- **`backward()`** â€” computes gradients for one position, accumulates cross-position KV gradients via `d_kv_cache`
- **`rmsnorm()` / `rmsnorm_bwd()`** â€” forward and backward for RMS normalization
- **`linear()` / `linear_bwd_w()` / `linear_bwd_x()`** â€” matrix multiply and its two gradient components
- **`generate()`** â€” autoregressive sampling with fresh KV cache per name

---

## Extending It

Ideas for building on this (roughly ordered by complexity):

| Extension | Difficulty | Description |
|---|---|---|
| CLI arguments | Easy | Add `--steps`, `--lr`, `--layers` via `std::env::args()` |
| Fetch `names.txt` | Easy | Download Karpathy's full dataset via `std::net::TcpStream` (still zero deps) |
| Checkpoint save/load | Easy | Write/read raw `f32` bytes via `std::fs` |
| Batched training | Medium | `[B, seq, embd]` tensors to amortize fixed overhead |
| SIMD matmul | Medium | `#[target_feature(enable = "avx2")]` on `linear()`, expect 4-8Ã— speedup |
| Gradient checkpointing | Medium | Recompute activations in backward for larger `N_LAYER` |
| Multi-layer scaling | Hard | Test with `N_EMBD=64`, `N_LAYER=4` â€” verify correctness at scale |

---

## License

[MIT](LICENSE)

---

<p align="center">
  Built by <a href="https://github.com/rustystack">rustystack</a> ğŸ¦€
</p>
